import json                                            # JSON ì…ì¶œë ¥ì„ ìœ„í•´ json ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
import math                                            # ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•´ math ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
import shutil                                          # í•„ìš” ì‹œ íŒŒì¼ ë³µì‚¬ë¥¼ ìœ„í•´ shutil ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from pathlib import Path                                # ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ Path í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from typing import List, Tuple, Dict                    # íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•´ typingì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤

import cv2                                             # í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ë° mp4 ìƒì„±ì„ ìœ„í•´ OpenCVë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤
import numpy as np                                     # ë°°ì—´ ì—°ì‚°ì„ ìœ„í•´ numpyë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤
import pandas as pd                                    # metadata.csv ì²˜ë¦¬ë¥¼ ìœ„í•´ pandasë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from tqdm.auto import tqdm                              # ì§„í–‰ í‘œì‹œë¥¼ ìœ„í•´ tqdm.autoì—ì„œ tqdmë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤

# ====== ê³ ì • ê²½ë¡œ ì„¤ì • ======
BASE_DIR = Path("/workspace/nas203/ds_RehabilitationMedicineData/IDs/Kimjihoo/3_project_HCCmove")  # í”„ë¡œì íŠ¸ ê¸°ë³¸ ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤
DATA_DIR = BASE_DIR / "data"                                                                        # ë°ì´í„° í´ë” ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤
CSV_PATH  = DATA_DIR / "metadata.csv"                                                               # ë©”íƒ€ë°ì´í„° CSV ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤

# ====== ìœ í‹¸ í•¨ìˆ˜: ìì—° ì •ë ¬ í‚¤ ======
def _natural_key(s: str) -> List:                                                                   # ìì—° ì •ë ¬ì„ ìœ„í•œ í‚¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    import re                                                                                       # ì •ê·œì‹ ì²˜ë¦¬ë¥¼ ìœ„í•´ re ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
    return [int(t) if t.isdigit() else t.lower() for t in re.split(r"(\d+)", s)]                    # ìˆ«ìì™€ ë¬¸ìë¥¼ ë¶„ë¦¬í•´ ì •ë ¬ í‚¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

# ====== ìœ í‹¸ í•¨ìˆ˜: ë¡œë²„ìŠ¤íŠ¸ z-score(MAD) ======
def robust_z(x: np.ndarray) -> np.ndarray:                                                          # MAD ê¸°ë°˜ ë¡œë²„ìŠ¤íŠ¸ z-scoreë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    med   = np.nanmedian(x)                                                                         # ë°ì´í„°ì˜ ì¤‘ì•™ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤
    mad   = np.nanmedian(np.abs(x - med))                                                           # ì¤‘ì•™ê°’ìœ¼ë¡œë¶€í„°ì˜ ì ˆëŒ€í¸ì°¨ì˜ ì¤‘ì•™ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤
    scale = 1.4826 * mad if mad > 0 else (np.nanstd(x) if np.nanstd(x) > 0 else 1.0)               # MADê°€ 0ì´ë©´ í‘œì¤€í¸ì°¨ ë˜ëŠ” 1.0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤
    return (x - med) / scale                                                                        # ìŠ¤ì¼€ì¼ë¡œ ë‚˜ëˆ„ì–´ ë¡œë²„ìŠ¤íŠ¸ z-scoreë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

# ====== ìœ í‹¸ í•¨ìˆ˜: LOCF ë³´ê°„ ======
def interpolate_locf(series: np.ndarray, is_outlier: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:  # LOCF ë³´ê°„ê³¼ í”Œë˜ê·¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    corrected = series.copy()                                                                        # ì›ë³¸ì„ ë³µì‚¬í•´ ë³´ì • ë°°ì—´ì„ ì¤€ë¹„í•©ë‹ˆë‹¤
    flags     = np.zeros_like(series, dtype=bool)                                                    # ë³´ê°„ ì—¬ë¶€ë¥¼ ê¸°ë¡í•  ë¶ˆë¦¬ì–¸ ë°°ì—´ì„ ìƒì„±í•©ë‹ˆë‹¤
    last_good = np.nan                                                                               # ë§ˆì§€ë§‰ ì •ìƒê°’ì„ NaNìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
    for t in range(len(series)):                                                                     # ì‹œê³„ì—´ì„ ì‹œê°„ ìˆœì„œë¡œ ìˆœíšŒí•©ë‹ˆë‹¤
        if is_outlier[t] or np.isnan(series[t]):                                                     # ì•„ì›ƒë¼ì´ì–´ì´ê±°ë‚˜ ê²°ì¸¡ì´ë©´ ë³´ê°„ ë¡œì§ì„ ì ìš©í•©ë‹ˆë‹¤
            if not np.isnan(last_good):                                                              # ì´ì „ ì •ìƒê°’ì´ ì¡´ì¬í•˜ë©´ LOCFë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤
                corrected[t] = last_good                                                             # ì´ì „ ì •ìƒê°’ì„ í˜„ì¬ ê°’ì— ë³µì‚¬í•©ë‹ˆë‹¤
                flags[t]    = True                                                                   # í•´ë‹¹ ì‹œì ì´ ë³´ê°„ë˜ì—ˆìŒì„ í‘œì‹œí•©ë‹ˆë‹¤
            else:                                                                                    # ì´ˆë°˜ë¶€ë¡œ ì´ì „ ì •ìƒê°’ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤
                corrected[t] = series[t]                                                             # ê°’ ë³€ê²½ ì—†ì´ ì›ë³¸ì„ ìœ ì§€í•©ë‹ˆë‹¤
        else:                                                                                        # í˜„ì¬ ê´€ì°°ì´ ì •ìƒì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤
            last_good = series[t]                                                                    # ë§ˆì§€ë§‰ ì •ìƒê°’ì„ í˜„ì¬ ê°’ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤
    return corrected, flags                                                                          # ë³´ì •ëœ ì‹œê³„ì—´ê³¼ ë³´ê°„ í”Œë˜ê·¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

# ====== ìœ í‹¸ í•¨ìˆ˜: ë§í¬/ìƒ‰ìƒ ì¶”ì¶œ ======
def get_coco_links(meta_info: Dict) -> Tuple[List[Tuple[int, int]], List[Tuple[int, int, int]]]:    # ìŠ¤ì¼ˆë ˆí†¤ ë§í¬ì™€ ìƒ‰ìƒì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    links  = meta_info.get("skeleton_links", [])                                                     # ë§í¬ ì¸ë±ìŠ¤ í˜ì–´ ëª©ë¡ì„ ì½ìŠµë‹ˆë‹¤
    colors = meta_info.get("skeleton_link_colors", [])                                               # ë§í¬ ìƒ‰ìƒ ëª©ë¡ì„ ì½ìŠµë‹ˆë‹¤
    return [tuple(map(int, e)) for e in links], [tuple(map(int, c)) for c in colors]                 # ì •ìˆ˜ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ë°˜í™˜í•©ë‹ˆë‹¤

# ====== ìœ í‹¸ í•¨ìˆ˜: í”„ë ˆì„ ì´ë¯¸ì§€ ëª©ë¡ ======
def list_frame_images(frame_dir: Path) -> List[Path]:                                                # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    exts  = (".jpg", ".jpeg", ".png", ".bmp")                                                        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ìë¥¼ ì •ì˜í•©ë‹ˆë‹¤
    files = [p for p in frame_dir.rglob("*") if p.suffix.lower() in exts]                            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    files.sort(key=lambda p: _natural_key(p.name))                                                   # ìì—° ì •ë ¬ í‚¤ë¡œ íŒŒì¼ ëª©ë¡ì„ ì •ë ¬í•©ë‹ˆë‹¤
    return files                                                                                     # ì •ë ¬ëœ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

# ====== ë¹„ë””ì˜¤ í•œ ê±´ ì²˜ë¦¬ í•¨ìˆ˜ ======
def process_one_row(row: pd.Series) -> None:
    """metadata.csvì˜ í•œ í–‰(row)ì„ ì…ë ¥ë°›ì•„ ë³´ì • ë° ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±"""
    # ====== ê¸°ë³¸ ê²½ë¡œ ì„¤ì • ======
    fps           = float(row["fps"]) if "fps" in row and not pd.isna(row["fps"]) else 30.0
    video_rel     = Path(str(row.get("video_path", "")).strip())
    keypoints_rel = Path(str(row.get("keypoints_path", "")).strip())
    frame_rel     = Path(str(row.get("frame_path", "")).strip())

    video_path     = (DATA_DIR / video_rel).resolve()
    keypoints_path = (DATA_DIR / keypoints_rel).resolve()
    frame_path     = (DATA_DIR / frame_rel).resolve()

    rel_after_raw_str = str(video_rel).split("0_RAW_DATA/")[-1]
    rel_after_raw     = Path(rel_after_raw_str)

    json_dir_rel = rel_after_raw.with_suffix("")
    OUT_JSON_DIR = DATA_DIR / "4_INTERP_DATA" / "JSON" / json_dir_rel
    OUT_JSON_DIR.mkdir(parents=True, exist_ok=True)

    mp4_dir_rel  = rel_after_raw.parent
    OUT_MP4_DIR  = DATA_DIR / "4_INTERP_DATA" / "MP4" / mp4_dir_rel
    OUT_MP4_DIR.mkdir(parents=True, exist_ok=True)
    out_mp4_path = OUT_MP4_DIR / f"{video_rel.stem}.mp4"

    # ====== ì…ë ¥ íŒŒì¼ í™•ì¸ ======
    if not keypoints_path.exists():
        raise FileNotFoundError(f"í‚¤í¬ì¸íŠ¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {keypoints_path}")
    if not frame_path.exists():
        raise FileNotFoundError(f"í”„ë ˆì„ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {frame_path}")

    json_files = sorted(keypoints_path.glob("*.json"), key=lambda p: _natural_key(p.name))
    if len(json_files) == 0:
        raise FileNotFoundError(f"JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {keypoints_path}")

    # ====== í‚¤í¬ì¸íŠ¸ ë¡œë“œ ======
    all_keypoints, all_scores, metas = [], [], None
    for jf in tqdm(json_files, desc="Load JSON", unit="file"):
        with open(jf, "r", encoding="utf-8") as f:
            data = json.load(f)
        inst = data["instance_info"][0]
        kps  = np.array(inst["keypoints"], dtype=np.float32)
        scs  = np.array(inst.get("keypoint_scores", [np.nan] * len(kps)), dtype=np.float32)
        all_keypoints.append(kps)
        all_scores.append(scs)
        if metas is None:
            metas = data.get("meta_info", {})

    K = np.stack(all_keypoints, axis=0)   # (T,17,2)
    C = np.stack(all_scores, axis=0)      # (T,17)
    Tlen = K.shape[0]
    joint_range = list(range(5, 17))

    # ====== Adaptive Velocity-based Outlier ë³´ì • ======
    def velocity_correct(x, y):
        """í”„ë ˆì„ê°„ ì†ë„ ê¸°ë°˜ìœ¼ë¡œ íŠ(outlier)ì„ ë³´ì •"""
        v = np.hypot(np.diff(x, prepend=x[0]), np.diff(y, prepend=y[0]))
        v_std, v_max = np.nanstd(v), np.nanmax(v)
        v_mean, v_med = np.nanmean(v), np.nanmedian(v)

        # Outlierê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš° (ì •ìƒ ë™ì‘)
        if v_std < 5 or v_max < 30:
            thr_vel = 9999.0
        else:
            thr_perc = np.nanpercentile(v, 95) * 1.2
            thr_mean = v_mean * 3.0
            thr_vel  = max(thr_perc, thr_mean, 80.0)

            out_mask = v > thr_vel
            if out_mask.mean() > 0.05:    # ê³¼íƒ ë°©ì§€
                thr_vel *= 1.5

        # ë³´ì • ë£¨í”„ (ì‹¤ì‹œê°„ ë°©ì‹)
        x_corr = x.copy()
        y_corr = y.copy()
        vel_prev_x, vel_prev_y = 0.0, 0.0
        for t in range(1, len(x)):
            dx, dy = x_corr[t] - x_corr[t-1], y_corr[t] - y_corr[t-1]
            vel = np.hypot(dx, dy)
            if vel > thr_vel:
                x_corr[t] = x_corr[t-1] + vel_prev_x
                y_corr[t] = y_corr[t-1] + vel_prev_y
            else:
                vel_prev_x, vel_prev_y = dx, dy
        return x_corr, y_corr, thr_vel

    thr_dict = {}
    for j in tqdm(joint_range, desc="Velocity correction", unit="joint"):
        x = K[:, j, 0].astype(float)
        y = K[:, j, 1].astype(float)
        x_corr, y_corr, thr_used = velocity_correct(x, y)
        K[:, j, 0], K[:, j, 1] = x_corr, y_corr
        thr_dict[j] = thr_used

    # ====== ë³´ì •ëœ JSON ì €ì¥ ======
    for t, jf in tqdm(list(enumerate(json_files)), total=Tlen, desc="Save JSON", unit="frame"):
        with open(jf, "r", encoding="utf-8") as f:
            data_in = json.load(f)
        inst_in  = data_in["instance_info"][0]
        kps_out  = []
        scs_out  = []
        for new_i, j in enumerate(joint_range):
            kps_out.append([float(K[t, j, 0]), float(K[t, j, 1])])
            if "keypoint_scores" in inst_in:
                scs_out.append(float(C[t, j]))
        meta_out = dict(data_in.get("meta_info", {}))
        links_in, colors_in = get_coco_links(meta_out)
        keep_idx   = [i for i, (a, b) in enumerate(links_in) if a >= 5 and b >= 5]
        links_new  = [(a - 5, b - 5) for i, (a, b) in enumerate(links_in) if i in keep_idx]
        colors_new = [colors_in[i] for i in keep_idx] if colors_in else []
        meta_out["skeleton_links"]       = links_new
        meta_out["skeleton_link_colors"] = colors_new
        meta_out["kept_joint_indices"]   = list(range(5, 17))
        meta_out["joint_index_offset"]   = 5
        data_out = {
            "meta_info": meta_out,
            "instance_info": [{
                "keypoints": kps_out,
                **({"keypoint_scores": scs_out} if scs_out else {}),
            }]
        }
        out_path = OUT_JSON_DIR / jf.name
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(data_out, f, ensure_ascii=False, indent=2)

    # ====== í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± ======
    frame_files = list_frame_images(frame_path)
    if len(frame_files) != Tlen:
        print(f"âš ï¸ í”„ë ˆì„ ìˆ˜({len(frame_files)})ì™€ JSON ìˆ˜({Tlen})ê°€ ë‹¤ë¦…ë‹ˆë‹¤")

    first_img = cv2.imread(str(frame_files[0]))
    if first_img is None:
        raise RuntimeError(f"í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {frame_files[0]}")
    h, w = first_img.shape[:2]

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    writer = cv2.VideoWriter(str(out_mp4_path), fourcc, fps, (w, h))

    links_all, link_colors = get_coco_links(metas or {})
    keep_idx_vis = [i for i, (a, b) in enumerate(links_all) if a >= 5 and b >= 5]
    links_vis = [(a, b) for i, (a, b) in enumerate(links_all) if i in keep_idx_vis]
    colors_vis = [link_colors[i] for i in keep_idx_vis] if link_colors else []

    for idx in tqdm(range(Tlen), desc="Render MP4", unit="frame"):
        img = cv2.imread(str(frame_files[idx])) if idx < len(frame_files) else np.zeros((h, w, 3), dtype=np.uint8)
        kps = K[idx]
        for j in range(5, 17):
            x_i, y_i = int(round(kps[j, 0])), int(round(kps[j, 1]))
            cv2.circle(img, (x_i, y_i), 3, (0, 255, 0), -1, lineType=cv2.LINE_AA)
        for (a, b), col in zip(links_vis, colors_vis if colors_vis else [(51, 153, 255)] * len(links_vis)):
            xa, ya = int(round(kps[a, 0])), int(round(kps[a, 1]))
            xb, yb = int(round(kps[b, 0])), int(round(kps[b, 1]))
            cv2.line(img, (xa, ya), (xb, yb),
                     (int(col[2]), int(col[1]), int(col[0])),
                     2, lineType=cv2.LINE_AA)
        writer.write(img)

    writer.release()


def main():                                                                                 # N01 ë‚´ë¶€ íŒŒì¼ë“¤ë§Œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤
    meta = pd.read_csv(CSV_PATH)                                                            # metadata.csvë¥¼ ë¡œë“œí•©ë‹ˆë‹¤
    if meta.empty:                                                                          # CSVê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤
        raise RuntimeError("metadata.csvê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")                                    # ë¹„ì–´ ìˆìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤

    # 'AI_dataset/N01' ì´ í¬í•¨ëœ í–‰ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤
    subset = meta[meta["video_path"].astype(str).str.contains("AI_dataset/N01", case=False, na=False)]

    if subset.empty:                                                                        # í•´ë‹¹ ê²½ë¡œë¥¼ ê°€ì§„ í–‰ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤
        raise RuntimeError("metadata.csvì—ì„œ 'AI_dataset/N01' ê²½ë¡œë¥¼ í¬í•¨í•œ ë¹„ë””ì˜¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

    total = len(subset)                                                                     # ì „ì²´ ì²˜ë¦¬ ëŒ€ìƒ ê°œìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
    print(f"ğŸ¯ ì´ {total}ê°œì˜ ë¹„ë””ì˜¤ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤ (AI_dataset/N01)")                     # ì„ íƒëœ ë¹„ë””ì˜¤ ê°œìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤

    # ìˆœì°¨ì ìœ¼ë¡œ ê° ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤
    for i, (_, row) in enumerate(subset.iterrows(), start=1):                               # ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ ì²˜ë¦¬ ì¸ë±ìŠ¤ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤
        print(f"\nâ–¶ ({i}/{total}) ì²˜ë¦¬ ì¤‘: {row['video_path']}")                            # í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í•©ë‹ˆë‹¤
        try:
            process_one_row(row)                                                            # ë¹„ë””ì˜¤ í•œ ê±´ ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤
        except Exception as e:                                                              # ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({row['video_path']}): {e}")                               # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
            continue                                                                        # ë‹¤ìŒ ë¹„ë””ì˜¤ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤


# ====== ì‹¤í–‰ë¶€ ======
if __name__ == "__main__":                                                                            # ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë™ì‘í•˜ë„ë¡ í•©ë‹ˆë‹¤
    main()                                                                                            # ë©”ì¸ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤
