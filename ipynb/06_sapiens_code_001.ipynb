{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff607a2",
   "metadata": {},
   "source": [
    "# 중간 산출물 존재하는 방식\n",
    "Frame, json, vis, output_mp4 총 4개의 산출물을 만들며 돌아가는 코드\n",
    "\n",
    "해당 코드에서 Frame, vis 같은 Frame 당 output을 만드는데 시간이 꽤나 오래걸림.\n",
    "\n",
    "추후, 수정 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6c95dd-9035-41b6-b751-0b86d3d2b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 1\n",
      "mmpose: 1.3.2 | mmdet: 3.2.0 | mmengine: 0.10.7 | mmcv: 2.1.0\n",
      "\n",
      "[PATH CHECK]\n",
      "FRAME_DIR    | DIR  | OK | ../data/Patient_data/new_code/frames/M01_VISIT2_상지_frame\n",
      "JSON_DIR     | DIR  | OK | ../data/Patient_data/new_code/json/M01_VISIT2_상지_json\n",
      "VIS_DIR      | DIR  | OK | ../data/Patient_data/new_code/vis/M01_VISIT2_상지_vis\n",
      "MP4_PATH     | FILE | OK | ../../../../data/김원 보산진 연구/M01_VISIT2_상지.MP4\n",
      "OUTPUT_MP4   | FILE | OK | ../data/Patient_data/new_code/output/M01_VISIT2_상지_output.mp4\n",
      "DET_CONFIG   | FILE | OK | ../sapiens/pose/demo/mmdetection_cfg/rtmdet_m_640-8xb32_coco-person_no_nms.py\n",
      "DET_CKPT     | FILE | OK | ../sapiens/pose/checkpoints/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\n",
      "POSE_CONFIG  | FILE | OK | ../sapiens/pose/configs/sapiens_pose/coco/sapiens_0.3b-210e_coco-1024x768.py\n",
      "POSE_CKPT    | FILE | OK | ../sapiens/pose/checkpoints/sapiens_0.3b/sapiens_0.3b_coco_best_coco_AP_796.pth\n"
     ]
    }
   ],
   "source": [
    "import os, sys, torch\n",
    "import mmpose, mmdet, mmengine, mmcv\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "print(\"mmpose:\", mmpose.__version__, \"| mmdet:\", mmdet.__version__, \"| mmengine:\", mmengine.__version__, \"| mmcv:\", mmcv.__version__)\n",
    "\n",
    "mp4_name = \"M01_VISIT2_상지\"  # 파일명만\n",
    "\n",
    "paths_tpl = {\n",
    "    \"MP4_PATH\":    \"../../../../data/김원 보산진 연구/{mp4_name}.MP4\",\n",
    "    \"FRAME_DIR\":   \"../data/Patient_data/new_code/frames/{mp4_name}_frame\",\n",
    "    \"JSON_DIR\":    \"../data/Patient_data/new_code/json/{mp4_name}_json\",\n",
    "    \"VIS_DIR\":     \"../data/Patient_data/new_code/vis/{mp4_name}_vis\",\n",
    "    \"OUTPUT_MP4\":  \"../data/Patient_data/new_code/output/{mp4_name}_output.mp4\",\n",
    "    \"DET_CONFIG\":  \"../sapiens/pose/demo/mmdetection_cfg/rtmdet_m_640-8xb32_coco-person_no_nms.py\",\n",
    "    \"DET_CKPT\":    \"../sapiens/pose/checkpoints/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\",\n",
    "    \"POSE_CONFIG\": \"../sapiens/pose/configs/sapiens_pose/coco/sapiens_0.3b-210e_coco-1024x768.py\",\n",
    "    \"POSE_CKPT\":   \"../sapiens/pose/checkpoints/sapiens_0.3b/sapiens_0.3b_coco_best_coco_AP_796.pth\",\n",
    "}\n",
    "\n",
    "# 1) {mp4_name} 치환\n",
    "paths = {k: v.format(mp4_name=mp4_name) for k, v in paths_tpl.items()}\n",
    "\n",
    "# 2) 디렉터리 생성 (치환 후에!)\n",
    "for k in (\"FRAME_DIR\", \"JSON_DIR\", \"VIS_DIR\"):\n",
    "    Path(paths[k]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (선택) OUTPUT_MP4는 파일이므로 부모 폴더만 보장\n",
    "Path(Path(paths[\"OUTPUT_MP4\"]).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3) 경로 체크\n",
    "print(\"\\n[PATH CHECK]\")\n",
    "for k in (\"FRAME_DIR\", \"JSON_DIR\", \"VIS_DIR\",\n",
    "          \"MP4_PATH\", \"OUTPUT_MP4\", \"DET_CONFIG\", \"DET_CKPT\", \"POSE_CONFIG\", \"POSE_CKPT\"):\n",
    "    p = paths[k]\n",
    "    exists = Path(p).exists()\n",
    "    typ = \"DIR \" if k in (\"FRAME_DIR\", \"JSON_DIR\", \"VIS_DIR\") else \"FILE\"\n",
    "    print(f\"{k:12s} | {typ} | {'OK' if exists else 'MISSING'} | {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4923e23-090c-490b-ba89-2a7293fc2cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CWD: /workspace/nas203/ds_RehabilitationMedicineData/IDs/Kimjihoo/3_project_HCCmove/ipynb\n",
      "[INFO] Video : /workspace/nas203/ds_RehabilitationMedicineData/data/김원 보산진 연구/M01_VISIT2_상지.MP4\n",
      "[INFO] Output: /workspace/nas203/ds_RehabilitationMedicineData/IDs/Kimjihoo/3_project_HCCmove/data/Patient_data/new_code/frames/M01_VISIT2_상지_frame\n",
      "FPS: 29.970 | 총 프레임: 11370 | 저장 예정: 11370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:   0% 30/11370 [00:05<37:50,  4.99frame/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     63\u001b[0m out_path \u001b[38;5;241m=\u001b[39m frame_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 64\u001b[0m ok \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMWRITE_JPEG_QUALITY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[1;32m     66\u001b[0m     saved \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- MP4_PATH → FRAME_DIR: 프레임 추출 저장 ---\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 기존 셀에서 만든 paths 사용\n",
    "mp4_path   = Path(paths[\"MP4_PATH\"])\n",
    "frame_dir  = Path(paths[\"FRAME_DIR\"])\n",
    "\n",
    "# (옵션) 앞 N초만 저장하고 싶으면 숫자를 넣고, 전체 저장은 None\n",
    "DURATION_SEC = None  # 예: 20\n",
    "\n",
    "# 대소문자 확장자 이슈 대비(대안 경로 자동 탐색)\n",
    "if not mp4_path.exists():\n",
    "    alt = mp4_path.with_suffix(\".mp4\") if mp4_path.suffix != \".mp4\" else mp4_path.with_suffix(\".MP4\")\n",
    "    if alt.exists():\n",
    "        print(f\"[INFO] 입력 비디오 대안 경로 사용: {alt}\")\n",
    "        mp4_path = alt\n",
    "\n",
    "print(\"[INFO] CWD:\", os.getcwd())\n",
    "print(\"[INFO] Video :\", mp4_path.resolve())\n",
    "print(\"[INFO] Output:\", frame_dir.resolve())\n",
    "\n",
    "cap = cv2.VideoCapture(str(mp4_path))\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"비디오를 열 수 없습니다: {mp4_path}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "total_prop = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "total_frames = int(total_prop) if total_prop and total_prop > 0 else -1\n",
    "\n",
    "# 저장할 목표 프레임 수 계산(전체 또는 앞 N초)\n",
    "if DURATION_SEC is not None and fps > 0:\n",
    "    target_frames = int(round(DURATION_SEC * fps))\n",
    "    if total_frames > 0:\n",
    "        target_frames = min(target_frames, total_frames)\n",
    "else:\n",
    "    target_frames = total_frames if total_frames > 0 else None\n",
    "\n",
    "print(f\"FPS: {fps:.3f} | 총 프레임: {total_frames} | 저장 예정: {target_frames if target_frames is not None else '전체(시간 기준)'}\")\n",
    "\n",
    "saved = 0\n",
    "idx = 0\n",
    "pbar_total = target_frames if target_frames is not None else (int(fps * (DURATION_SEC or 10)) or 100)\n",
    "pbar = tqdm(total=pbar_total, desc=\"Extracting frames\", unit=\"frame\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 시간 제한 모드일 때( FPS를 못 읽는 경우 포함 )\n",
    "    pos_msec = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "    if target_frames is not None:\n",
    "        if idx >= target_frames:\n",
    "            break\n",
    "    elif DURATION_SEC is not None:\n",
    "        if pos_msec >= DURATION_SEC * 1000:\n",
    "            break\n",
    "\n",
    "    out_path = frame_dir / f\"frame_{idx:06d}.jpg\"\n",
    "    ok = cv2.imwrite(str(out_path), frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "    if ok:\n",
    "        saved += 1\n",
    "        if target_frames is not None:\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            # 시간 기준일 때 대략 진행률\n",
    "            pbar.update(1 if pbar.n < pbar.total else 0)\n",
    "    idx += 1\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "print(f\"완료: {saved}장 저장 → {frame_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae96644-a8b4-4b53-afd5-2eefc39c434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mmengine/utils/package_utils.py:48: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "/opt/conda/lib/python3.10/importlib/__init__.py:169: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  _bootstrap._exec(spec, module)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ../sapiens/pose/checkpoints/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\n",
      "Loads checkpoint by local backend from path: ../sapiens/pose/checkpoints/sapiens_0.3b/sapiens_0.3b_coco_best_coco_AP_796.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "missing keys in source state_dict: head.deconv_layers.1.weight, head.deconv_layers.1.bias, head.deconv_layers.1.running_mean, head.deconv_layers.1.running_var, head.deconv_layers.4.weight, head.deconv_layers.4.bias, head.deconv_layers.4.running_mean, head.deconv_layers.4.running_var, head.conv_layers.1.weight, head.conv_layers.1.bias, head.conv_layers.1.running_mean, head.conv_layers.1.running_var, head.conv_layers.4.weight, head.conv_layers.4.bias, head.conv_layers.4.running_mean, head.conv_layers.4.running_var\n",
      "\n",
      "모델 초기화 완료 ✅\n"
     ]
    }
   ],
   "source": [
    "# 앞 셀에서 paths를 만든 상태여야 함\n",
    "assert 'paths' in globals(), \"앞 셀에서 paths를 먼저 생성하세요.\"\n",
    "\n",
    "import mmpretrain  # VisionTransformer 등록\n",
    "\n",
    "import mmcv\n",
    "from pathlib import Path\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmpose.apis import init_model as init_pose_estimator, inference_topdown\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.structures import merge_data_samples, split_instances\n",
    "\n",
    "# paths 딕셔너리에서 경로 바인딩\n",
    "DET_CONFIG  = paths[\"DET_CONFIG\"]\n",
    "DET_CKPT    = paths[\"DET_CKPT\"]\n",
    "POSE_CONFIG = paths[\"POSE_CONFIG\"]\n",
    "POSE_CKPT   = paths[\"POSE_CKPT\"]\n",
    "\n",
    "# (선택) 프레임 폴더도 같이 써야 한다면\n",
    "DATA_DIR = paths[\"FRAME_DIR\"]\n",
    "\n",
    "# (권장) 존재 여부 체크\n",
    "for k in (\"DET_CONFIG\", \"DET_CKPT\", \"POSE_CONFIG\", \"POSE_CKPT\"):\n",
    "    p = Path(globals()[k])\n",
    "    assert p.exists(), f\"{k} 경로를 찾을 수 없습니다: {p}\"\n",
    "\n",
    "# detector\n",
    "detector = init_detector(DET_CONFIG, DET_CKPT, device=\"cuda:0\")\n",
    "detector.cfg = adapt_mmdet_pipeline(detector.cfg)\n",
    "\n",
    "# pose estimator (override_ckpt_meta 제거)\n",
    "pose_estimator = init_pose_estimator(\n",
    "    POSE_CONFIG, POSE_CKPT,\n",
    "    device=\"cuda:0\",\n",
    "    cfg_options=dict(model=dict(test_cfg=dict(output_heatmaps=False)))\n",
    ")\n",
    "\n",
    "# 시각화기\n",
    "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
    "visualizer.set_dataset_meta(pose_estimator.dataset_meta, skeleton_style=\"mmpose\")\n",
    "\n",
    "print(\"모델 초기화 완료 ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d81768-ed2c-4c75-9765-21263cadf5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 31장 → 15fps용 16장 선택\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0% 0/16 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "  6% 1/16 [00:01<00:21,  1.43s/it]\u001b[A\n",
      " 12% 2/16 [00:02<00:13,  1.07it/s]\u001b[A\n",
      " 19% 3/16 [00:02<00:10,  1.27it/s]\u001b[A\n",
      " 25% 4/16 [00:03<00:08,  1.36it/s]\u001b[A\n",
      " 31% 5/16 [00:03<00:07,  1.48it/s]\u001b[A\n",
      " 38% 6/16 [00:04<00:06,  1.53it/s]\u001b[A\n",
      " 44% 7/16 [00:05<00:05,  1.58it/s]\u001b[A\n",
      " 50% 8/16 [00:05<00:04,  1.62it/s]\u001b[A\n",
      " 56% 9/16 [00:06<00:04,  1.63it/s]\u001b[A\n",
      " 62% 10/16 [00:06<00:03,  1.66it/s]\u001b[A\n",
      " 69% 11/16 [00:07<00:03,  1.66it/s]\u001b[A\n",
      " 75% 12/16 [00:08<00:02,  1.65it/s]\u001b[A\n",
      " 81% 13/16 [00:08<00:01,  1.61it/s]\u001b[A\n",
      " 88% 14/16 [00:09<00:01,  1.61it/s]\u001b[A\n",
      " 94% 15/16 [00:09<00:00,  1.64it/s]\u001b[A\n",
      "100% 16/16 [00:10<00:00,  1.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료. 성공 16, 실패 0, 출력: ../data/Patient_data/new_code/json/M01_VISIT2_상지_json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 30→15fps 다운샘플 + 주기적 검출 + 이전 포즈 bbox 재사용 ---\n",
    "import os, re, glob, json, numpy as np, mmcv, cv2\n",
    "from tqdm import tqdm\n",
    "from mmdet.apis import inference_detector\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.structures import merge_data_samples, split_instances\n",
    "\n",
    "# paths 딕셔너리에서 경로 가져오기 (앞 셀에서 paths 생성되어 있어야 함)\n",
    "assert 'paths' in globals(), \"앞 셀에서 paths를 먼저 생성하세요.\"\n",
    "FRAME_DIR = paths[\"FRAME_DIR\"]\n",
    "JSON_DIR  = paths[\"JSON_DIR\"]\n",
    "os.makedirs(JSON_DIR, exist_ok=True)  # JSON 저장 폴더 보장\n",
    "\n",
    "def natural_key(s: str):\n",
    "    base = os.path.basename(s)\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', base)]\n",
    "\n",
    "all_frames = sorted(\n",
    "    [p for p in glob.glob(os.path.join(FRAME_DIR, \"*\"))\n",
    "     if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))],\n",
    "    key=natural_key)\n",
    "assert all_frames, f\"이미지 없음: {FRAME_DIR}\"\n",
    "frames = all_frames[::2]  # 30fps → 15fps\n",
    "print(f\"전체 {len(all_frames)}장 → 15fps용 {len(frames)}장 선택\")\n",
    "\n",
    "def to_py(obj):\n",
    "    import numpy as _np\n",
    "    if isinstance(obj, _np.ndarray): return obj.tolist()\n",
    "    if isinstance(obj, (_np.floating,)): return float(obj)\n",
    "    if isinstance(obj, (_np.integer,)):  return int(obj)\n",
    "    if isinstance(obj, dict):  return {k: to_py(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)): return [to_py(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "def clip_xyxy(xyxy, w, h):\n",
    "    x1, y1, x2, y2 = xyxy\n",
    "    return [max(0, x1), max(0, y1), min(w - 1, x2), min(h - 1, y2)]\n",
    "\n",
    "def bbox_from_keypoints(kpts, scores, thr=0.3, pad_scale=1.25, img_wh=None):\n",
    "    valid = scores >= thr\n",
    "    if valid.sum() == 0:\n",
    "        return None\n",
    "    xy = kpts[valid]\n",
    "    x1, y1 = xy.min(axis=0)\n",
    "    x2, y2 = xy.max(axis=0)\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    w, h = max(2.0, x2 - x1), max(2.0, y2 - y1)\n",
    "    w2, h2 = w * pad_scale / 2, h * pad_scale / 2\n",
    "    bx = [cx - w2, cy - h2, cx + w2, cy + h2]\n",
    "    if img_wh is not None:\n",
    "        bx = clip_xyxy(bx, img_wh[0], img_wh[1])\n",
    "    return bx\n",
    "\n",
    "det_every = 5\n",
    "det_thr   = 0.5\n",
    "nms_thr   = 0.5\n",
    "kpt_thr   = 0.3\n",
    "pad_scale = 1.30\n",
    "miss_max  = 2\n",
    "\n",
    "prev_bbox = None\n",
    "miss_cnt  = 0\n",
    "ok, fail  = 0, 0\n",
    "\n",
    "for i, img_path in enumerate(tqdm(frames)):\n",
    "    try:\n",
    "        img_bgr = cv2.imread(img_path)\n",
    "        if img_bgr is None:\n",
    "            continue\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        need_det = (i % det_every == 0) or (prev_bbox is None) or (miss_cnt > miss_max)\n",
    "        if need_det:\n",
    "            det = inference_detector(detector, img_rgb)\n",
    "            pred = det.pred_instances.cpu().numpy()\n",
    "            if len(pred.bboxes):\n",
    "                bbs = np.concatenate((pred.bboxes, pred.scores[:, None]), axis=1)\n",
    "                keep = (pred.labels == 0) & (pred.scores > det_thr)\n",
    "                bbs = bbs[keep]\n",
    "                if len(bbs) > 0:\n",
    "                    bbs = bbs[nms(bbs, nms_thr), :4]\n",
    "                if len(bbs) > 0:\n",
    "                    areas = (bbs[:, 2] - bbs[:, 0]) * (bbs[:, 3] - bbs[:, 1])\n",
    "                    prev_bbox = bbs[np.argmax(areas)].tolist()\n",
    "                    miss_cnt = 0\n",
    "                else:\n",
    "                    prev_bbox = None\n",
    "            else:\n",
    "                prev_bbox = None\n",
    "\n",
    "        bboxes_np = (np.array([prev_bbox], dtype=np.float32)\n",
    "                     if prev_bbox is not None else np.empty((0, 4), dtype=np.float32))\n",
    "\n",
    "        pose_results = inference_topdown(pose_estimator, img_rgb, bboxes_np)\n",
    "        data_sample  = merge_data_samples(pose_results)\n",
    "\n",
    "        inst = data_sample.get('pred_instances', None)\n",
    "        if inst is not None and len(inst.get('keypoints', [])) > 0:\n",
    "            kpts_all    = inst['keypoints']\n",
    "            kscores_all = inst['keypoint_scores']\n",
    "            idx = int(np.argmax(kscores_all.mean(axis=1)))\n",
    "            kpts, kscores = kpts_all[idx], kscores_all[idx]\n",
    "            nb = bbox_from_keypoints(kpts, kscores, thr=kpt_thr, pad_scale=pad_scale, img_wh=(W, H))\n",
    "            if nb is not None:\n",
    "                prev_bbox = nb\n",
    "                miss_cnt = 0\n",
    "            else:\n",
    "                miss_cnt += 1\n",
    "        else:\n",
    "            miss_cnt += 1\n",
    "\n",
    "        # JSON 저장\n",
    "        if inst is not None:\n",
    "            inst_list = split_instances(inst)\n",
    "            payload = dict(meta_info=pose_estimator.dataset_meta, instance_info=inst_list)\n",
    "            base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            json_path = os.path.join(JSON_DIR, base + \".json\")\n",
    "            with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(to_py(payload), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        ok += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        fail += 1\n",
    "        print(\"에러:\", os.path.basename(img_path), \"->\", e)\n",
    "\n",
    "print(f\"완료. 성공 {ok}, 실패 {fail}, 출력: {JSON_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f78f31-6bdf-4e9e-b743-99b0f4e874b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlay 30fps:   0% 11/11370 [00:02<44:51,  4.22frame/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m         json_data \u001b[38;5;241m=\u001b[39m last_json_data\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 원본 프레임 로드\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m img_bgr \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_bgr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     skipped \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 15fps 예측(JSON) → 30fps 업샘플: JSON 저장 없이 '오버레이 이미지'만 저장 ---\n",
    "import os, re, glob, json            # 경로/정렬/파일검색/JSON\n",
    "import cv2                           # OpenCV (이미지 로드/저장, 그리기)\n",
    "import numpy as np                   # 수치 계산\n",
    "from tqdm import tqdm                # 진행률 표시\n",
    "\n",
    "VIS_DIR=  paths[\"VIS_DIR\"]\n",
    "\n",
    "# ===== 자연 정렬 =====\n",
    "def natural_key(s: str):\n",
    "    base = os.path.basename(s)\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', base)]\n",
    "\n",
    "# ===== 30fps 전체 프레임 목록 =====\n",
    "all_frames = sorted(\n",
    "    [p for p in glob.glob(os.path.join(FRAME_DIR, \"*\")) if p.lower().endswith((\".jpg\",\".png\",\".jpeg\"))],\n",
    "    key=natural_key\n",
    ")\n",
    "assert all_frames, f\"원본 프레임이 없습니다: {FRAME_DIR}\"\n",
    "\n",
    "# ===== 15fps JSON 맵 =====\n",
    "src_json_files = sorted(glob.glob(os.path.join(JSON_DIR, \"*.json\")), key=natural_key)\n",
    "assert src_json_files, f\"15fps JSON이 없습니다: {JSON_DIR}\"\n",
    "src_json_map = {os.path.splitext(os.path.basename(p))[0]: p for p in src_json_files}\n",
    "\n",
    "# ===== 시각화 파라미터 (단일 색) =====\n",
    "KPT_THR         = 0.05                # 키포인트 신뢰도 임계값 (필요시 0.2~0.3으로 올려 노이즈 제거)\n",
    "KEYPOINT_COLOR  = (0, 255, 0)         # 점 색 (BGR)\n",
    "SKELETON_COLOR  = (255, 128, 0)       # 선 색 (BGR)\n",
    "RADIUS          = 4                   # 점 반지름\n",
    "THICKNESS       = 2                   # 선 두께\n",
    "ANTI_ALIAS      = cv2.LINE_AA         # 안티앨리어싱\n",
    "\n",
    "# ===== 스켈레톤 링크 얻기 =====\n",
    "def get_links(meta: dict):\n",
    "    if \"skeleton_links\" in meta:\n",
    "        return meta[\"skeleton_links\"]\n",
    "    if \"skeleton\" in meta:\n",
    "        return meta[\"skeleton\"]\n",
    "    return []\n",
    "\n",
    "# ===== 한 인스턴스 그리기 (단일 색, bbox X) =====\n",
    "def draw_instance_uniform(img_bgr, instance: dict, links, kpt_thr=0.05):\n",
    "    kpts = np.array(instance[\"keypoints\"], dtype=np.float32)\n",
    "    ksc  = np.array(instance[\"keypoint_scores\"], dtype=np.float32)\n",
    "    # 키포인트(점)\n",
    "    for xy, sc in zip(kpts, ksc):\n",
    "        if sc < kpt_thr:\n",
    "            continue\n",
    "        x, y = int(xy[0]), int(xy[1])\n",
    "        cv2.circle(img_bgr, (x, y), RADIUS, KEYPOINT_COLOR, -1, lineType=ANTI_ALIAS)\n",
    "    # 스켈레톤(선)\n",
    "    for link in links:\n",
    "        if isinstance(link, dict) and \"link\" in link:\n",
    "            i, j = link[\"link\"]\n",
    "        elif isinstance(link, (list, tuple)) and len(link) == 2:\n",
    "            i, j = link\n",
    "        else:\n",
    "            continue\n",
    "        if i >= len(kpts) or j >= len(kpts):\n",
    "            continue\n",
    "        if ksc[i] < kpt_thr or ksc[j] < kpt_thr:\n",
    "            continue\n",
    "        p1 = (int(kpts[i][0]), int(kpts[i][1]))\n",
    "        p2 = (int(kpts[j][0]), int(kpts[j][1]))\n",
    "        cv2.line(img_bgr, p1, p2, SKELETON_COLOR, THICKNESS, lineType=ANTI_ALIAS)\n",
    "\n",
    "# ===== 메인: 30fps 모든 프레임에 대해 오버레이 이미지 저장 (JSON은 저장하지 않음) =====\n",
    "last_json_data = None\n",
    "written_img, skipped = 0, 0\n",
    "\n",
    "for fpath in tqdm(all_frames, desc=\"Overlay 30fps\", unit=\"frame\"):\n",
    "    base = os.path.splitext(os.path.basename(fpath))[0]           # 예: frame_000123\n",
    "    # 15fps JSON 있으면 로드, 없으면 직전 JSON을 사용(Zero-Order Hold)\n",
    "    src_json_path = src_json_map.get(base, None)\n",
    "    if src_json_path is not None:\n",
    "        with open(src_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "        last_json_data = json_data\n",
    "    else:\n",
    "        if last_json_data is None:\n",
    "            # (초반부 예외 처리) 앞으로 보이는 첫 JSON 1회용 사용\n",
    "            next_json_path = None\n",
    "            for nb in sorted(src_json_map.keys(), key=natural_key):\n",
    "                if nb > base:\n",
    "                    next_json_path = src_json_map[nb]\n",
    "                    break\n",
    "            if next_json_path is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            with open(next_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "        else:\n",
    "            json_data = last_json_data\n",
    "\n",
    "    # 원본 프레임 로드\n",
    "    img_bgr = cv2.imread(fpath)\n",
    "    if img_bgr is None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # 스켈레톤/키포인트 그리기\n",
    "    meta      = json_data.get(\"meta_info\", {})\n",
    "    instances = json_data.get(\"instance_info\", [])\n",
    "    links     = get_links(meta)\n",
    "    for inst in instances:\n",
    "        draw_instance_uniform(img_bgr, inst, links, kpt_thr=KPT_THR)\n",
    "\n",
    "    # 오버레이 이미지 저장\n",
    "    out_img_path = os.path.join(VIS_DIR, base + \".jpg\")\n",
    "    if cv2.imwrite(out_img_path, img_bgr):\n",
    "        written_img += 1\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"완료: 오버레이 이미지 {written_img}개 저장, 스킵 {skipped}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60ff50-be35-4b54-b423-b9b6dd8e17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 업샘플된 30fps 시각화 프레임 → 30fps MP4 (tqdm 진행률, 지정 경로 저장) ---\n",
    "import os, re, glob, cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert 'paths' in globals(), \"앞 셀에서 paths를 먼저 생성하세요.\"\n",
    "\n",
    "# 프레임 폴더(VIS_DIR)와 출력 파일 경로(OUTPUT_MP4)를 paths에서 사용\n",
    "VIS_DIR  = paths[\"VIS_DIR\"]\n",
    "SAVE_MP4 = paths[\"OUTPUT_MP4\"]\n",
    "Path(Path(SAVE_MP4).parent).mkdir(parents=True, exist_ok=True)  # 출력 폴더 보장\n",
    "\n",
    "def natural_key(s: str):\n",
    "    base = os.path.basename(s)\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', base)]\n",
    "\n",
    "# 프레임 수집 (자연 정렬)\n",
    "frames = sorted(\n",
    "    [p for p in glob.glob(os.path.join(VIS_DIR, \"*\")) if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))],\n",
    "    key=natural_key\n",
    ")\n",
    "assert frames, f\"프레임 이미지가 없습니다: {VIS_DIR}\"\n",
    "\n",
    "# 비디오 라이터 초기화\n",
    "first = cv2.imread(frames[0])\n",
    "assert first is not None, f\"첫 프레임 로드 실패: {frames[0]}\"\n",
    "h, w = first.shape[:2]\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # 호환성 좋은 코덱\n",
    "writer = cv2.VideoWriter(SAVE_MP4, fourcc, 30, (w, h))\n",
    "\n",
    "# 인코딩 루프 (tqdm 진행률)\n",
    "for f in tqdm(frames, desc=\"Encoding 30fps MP4\", unit=\"frame\"):\n",
    "    img = cv2.imread(f)\n",
    "    if img is None:\n",
    "        print(f\"[경고] 프레임 로드 실패: {f}\")\n",
    "        continue\n",
    "    if img.shape[:2] != (h, w):\n",
    "        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "    writer.write(img)\n",
    "\n",
    "writer.release()\n",
    "print(\"30fps 비디오 저장 완료:\", SAVE_MP4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ee56be-f7b8-41be-823a-d202ca1dd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/mmengine/utils/package_utils.py:48: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📂 Paths\n",
      "============================================================\n",
      "MP4_PATH     : ../../../../data/김원 보산진 연구/M01_VISIT2_상지.MP4  ✅\n",
      "FRAME_DIR    : ../data/Patient_data/new_code/frames/M01_VISIT2_상지_frame  ✅\n",
      "JSON_DIR     : ../data/Patient_data/new_code/json/M01_VISIT2_상지_json  ✅\n",
      "VIS_DIR      : ../data/Patient_data/new_code/vis/M01_VISIT2_상지_vis  ✅\n",
      "OUTPUT_MP4   : ../data/Patient_data/new_code/output/M01_VISIT2_상지_output.mp4  ✅\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass-1→FixedBBox:   6% 102/1798 [00:03<00:54, 30.89frame/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 246\u001b[0m\n\u001b[1;32m    244\u001b[0m print_paths(paths)  \u001b[38;5;66;03m# 경로 상태를 눈으로 확인할 수 있게 출력합니다.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m yolo \u001b[38;5;241m=\u001b[39m YOLO(YOLO_WEIGHTS)  \u001b[38;5;66;03m# YOLOv8 모델을 초기화합니다.\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m fixed_box, orig_fps, (W, H), frame_limit \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_fixed_box_fullscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 전체 프레임을 스캔해 고정 박스를 산출합니다.\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMP4_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDURATION_SEC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESIZE_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYOLO_CONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFIXED_BOX_MARGIN\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 스캔에 사용할 설정을 전달합니다.\u001b[39;49;00m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 고정 박스 산출 호출을 마칩니다.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Fixed BBox (orig): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfixed_box\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 원본 해상도 기준 고정 박스를 출력합니다.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m n_written \u001b[38;5;241m=\u001b[39m run_pose_and_render(  \u001b[38;5;66;03m# 샘플링된 프레임만 포즈/JSON/오버레이/비디오를 수행합니다.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     MP4_PATH, paths, fixed_box, orig_fps, frame_limit, TARGET_FPS, RESIZE_WIDTH, SAVE_FRAMES, SAVE_OVERLAY_IMAGES  \u001b[38;5;66;03m# 본 처리 설정을 전달합니다.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m )  \u001b[38;5;66;03m# 본 처리 호출을 마칩니다.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mcompute_fixed_box_fullscan\u001b[0;34m(mp4_path, yolo_model, duration_sec, resize_width, conf_thr, margin_ratio)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# 프레임을 더 이상 읽을 수 없으면 루프를 종료합니다.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m small, sx, sy \u001b[38;5;241m=\u001b[39m resize_keep_w(frame, new_w\u001b[38;5;241m=\u001b[39mresize_width)  \u001b[38;5;66;03m# YOLO 속도/일관성을 위해 720p 수준으로 리사이즈합니다.\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf_thr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 사람 클래스만 대상으로 YOLO 검출을 실행합니다.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m xyxy \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mboxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 검출 결과를 안전하게 numpy 배열로 변환합니다.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xyxy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 하나 이상의 박스가 존재하면\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:187\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:229\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:332\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 332\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    180\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    183\u001b[0m )\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:637\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 637\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:93\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 15fps 예측(JSON) → 30fps 업샘플: '오버레이 이미지' 저장 옵션 + tqdm 진행률로 바로 MP4 인코딩 ---\n",
    "import os, re, glob, json                              # 경로/정렬/파일검색/JSON 파싱을 위한 표준 라이브러리를 임포트합니다.\n",
    "import cv2                                             # 이미지 로드/그리기/비디오 인코딩을 위해 OpenCV를 임포트합니다.\n",
    "import numpy as np                                     # 수치 연산을 위해 NumPy를 임포트합니다.\n",
    "from tqdm import tqdm                                   # 진행률 표시를 위해 tqdm을 임포트합니다.\n",
    "from pathlib import Path                                # 출력 디렉터리 보장을 위해 Path를 임포트합니다.\n",
    "import time                                            # 처리 속도(FPS) 측정을 위해 time 모듈을 임포트합니다.\n",
    "\n",
    "assert 'paths' in globals(), \"앞 셀에서 paths를 먼저 생성하세요.\"  # 상위 셀에서 경로 딕셔너리(paths)가 정의되었는지 확인합니다.\n",
    "FRAME_DIR = paths[\"FRAME_DIR\"]                         # 원본 30fps 프레임 폴더 경로를 가져옵니다.\n",
    "JSON_DIR  = paths[\"JSON_DIR\"]                          # 15fps JSON 폴더 경로를 가져옵니다.\n",
    "VIS_DIR   = paths[\"VIS_DIR\"]                           # (선택) 오버레이 이미지를 저장할 폴더 경로를 가져옵니다.\n",
    "SAVE_MP4  = paths[\"OUTPUT_MP4\"]                        # 최종 출력 MP4 파일 경로를 가져옵니다.\n",
    "\n",
    "SAVE_OVERLAY_IMAGES = False                            # 오버레이 이미지를 파일로도 저장할지 여부(False: 저장 안 함, True: 저장)를 설정합니다.\n",
    "FPS_OUT = 30                                           # 출력 비디오 FPS를 설정합니다.\n",
    "\n",
    "# ===== 자연 정렬 유틸 =====\n",
    "def natural_key(s: str):                               # 파일명을 사람 친화적으로 정렬하기 위한 키를 생성하는 함수를 정의합니다.\n",
    "    base = os.path.basename(s)                         # 경로에서 파일명만 분리합니다.\n",
    "    return [int(t) if t.isdigit() else t.lower()       # 숫자는 정수로 변환, 나머지는 소문자 문자열로 변환하여\n",
    "            for t in re.split(r'(\\d+)', base)]         # 숫자/문자 경계 기준으로 분리된 토큰 리스트를 반환합니다.\n",
    "\n",
    "# ===== 스켈레톤 링크 추출 =====\n",
    "def get_links(meta: dict):                             # JSON 메타 정보에서 스켈레톤 연결 정보를 얻는 함수를 정의합니다.\n",
    "    if \"skeleton_links\" in meta:                       # 새 키 이름(skeleton_links)이 존재하면\n",
    "        return meta[\"skeleton_links\"]                  # 해당 값을 반환합니다.\n",
    "    if \"skeleton\" in meta:                             # 구 키 이름(skeleton)이 존재하면\n",
    "        return meta[\"skeleton\"]                        # 해당 값을 반환합니다.\n",
    "    return []                                          # 없으면 빈 리스트를 반환합니다.\n",
    "\n",
    "# ===== 시각화 파라미터 (단일 색) =====\n",
    "KPT_THR        = 0.05                                  # 키포인트 신뢰도 임계값(낮은 점 필터링)을 설정합니다.\n",
    "KEYPOINT_COLOR = (0, 255, 0)                           # 키포인트 점 색상(BGR, 녹색)을 지정합니다.\n",
    "SKELETON_COLOR = (255, 128, 0)                         # 스켈레톤 선 색상(BGR, 주황)을 지정합니다.\n",
    "RADIUS         = 4                                     # 키포인트 점 반지름을 지정합니다.\n",
    "THICKNESS      = 2                                     # 스켈레톤 선 두께를 지정합니다.\n",
    "ANTI_ALIAS     = cv2.LINE_AA                           # 안티앨리어싱 라인 타입을 사용합니다.\n",
    "\n",
    "# ===== 단일 인스턴스 오버레이 그리기 =====\n",
    "def draw_instance_uniform(img_bgr, instance: dict, links, kpt_thr=0.05):  # 한 인스턴스의 키포인트/스켈레톤을 그리는 함수를 정의합니다.\n",
    "    kpts = np.array(instance[\"keypoints\"], dtype=np.float32)               # 키포인트 좌표 배열을 float32로 변환합니다.\n",
    "    ksc  = np.array(instance[\"keypoint_scores\"], dtype=np.float32)         # 키포인트 신뢰도 배열을 float32로 변환합니다.\n",
    "    for xy, sc in zip(kpts, ksc):                                          # 각 키포인트 좌표와 신뢰도를 순회합니다.\n",
    "        if sc < kpt_thr:                                                   # 신뢰도가 임계값보다 낮으면\n",
    "            continue                                                       # 그리기를 건너뜁니다.\n",
    "        x, y = int(xy[0]), int(xy[1])                                      # 좌표를 정수 픽셀 값으로 변환합니다.\n",
    "        cv2.circle(img_bgr, (x, y), RADIUS, KEYPOINT_COLOR, -1, lineType=ANTI_ALIAS)  # 채워진 원으로 점을 그립니다.\n",
    "    for link in links:                                                     # 링크 정의를 순회하며 선을 그립니다.\n",
    "        if isinstance(link, dict) and \"link\" in link:                      # 딕셔너리 형태 { \"link\": [i, j] }를 처리합니다.\n",
    "            i, j = link[\"link\"]                                            # 인덱스 쌍을 추출합니다.\n",
    "        elif isinstance(link, (list, tuple)) and len(link) == 2:           # [i, j] 또는 (i, j) 형태를 처리합니다.\n",
    "            i, j = link                                                    # 인덱스 쌍을 가져옵니다.\n",
    "        else:                                                              # 그 외 형식은\n",
    "            continue                                                       # 무시합니다.\n",
    "        if i >= len(kpts) or j >= len(kpts):                               # 인덱스가 범위를 벗어나면\n",
    "            continue                                                       # 그리기를 건너뜁니다.\n",
    "        if ksc[i] < kpt_thr or ksc[j] < kpt_thr:                           # 연결된 점 중 하나라도 임계값 미만이면\n",
    "            continue                                                       # 선을 그리지 않습니다.\n",
    "        p1 = (int(kpts[i][0]), int(kpts[i][1]))                            # 첫 번째 점 좌표를 정수로 준비합니다.\n",
    "        p2 = (int(kpts[j][0]), int(kpts[j][1]))                            # 두 번째 점 좌표를 정수로 준비합니다.\n",
    "        cv2.line(img_bgr, p1, p2, SKELETON_COLOR, THICKNESS, lineType=ANTI_ALIAS)  # 두 점을 선으로 연결해 그립니다.\n",
    "\n",
    "# ===== 30fps 전체 프레임 수집 =====\n",
    "all_frames = sorted(                                                       # 모든 입력 프레임을 자연 정렬로 수집합니다.\n",
    "    [p for p in glob.glob(os.path.join(FRAME_DIR, \"*\"))                    # 폴더 내 모든 항목 중에서\n",
    "     if p.lower().endswith((\".jpg\", \".png\", \".jpeg\"))],                    # 이미지 확장자만 필터링합니다.\n",
    "    key=natural_key                                                        # 자연 정렬 키를 사용합니다.\n",
    ")\n",
    "assert all_frames, f\"원본 프레임이 없습니다: {FRAME_DIR}\"                   # 프레임이 없으면 실행을 중단합니다.\n",
    "\n",
    "# ===== 15fps JSON 맵 구성 =====\n",
    "src_json_files = sorted(                                                   # 15fps JSON 파일 목록을 자연 정렬로 수집합니다.\n",
    "    glob.glob(os.path.join(JSON_DIR, \"*.json\")),                           # JSON 폴더에서 *.json 파일을 찾습니다.\n",
    "    key=natural_key                                                        # 자연 정렬 키를 사용합니다.\n",
    ")\n",
    "assert src_json_files, f\"15fps JSON이 없습니다: {JSON_DIR}\"                 # JSON이 없으면 실행을 중단합니다.\n",
    "src_json_map = {os.path.splitext(os.path.basename(p))[0]: p                # 파일명(확장자 제외)을 키로 하고\n",
    "                for p in src_json_files}                                   # 전체 경로를 값으로 하는 맵을 생성합니다.\n",
    "sorted_json_keys = sorted(src_json_map.keys(), key=natural_key)            # 자연 정렬된 JSON 키 리스트를 미리 만들어 둡니다.\n",
    "\n",
    "# ===== 비디오 라이터 초기화 =====\n",
    "first_img = cv2.imread(all_frames[0])                                      # 첫 프레임을 로드해 해상도를 파악합니다.\n",
    "assert first_img is not None, f\"첫 프레임 로드 실패: {all_frames[0]}\"     # 첫 프레임 로드 실패 시 중단합니다.\n",
    "H, W = first_img.shape[:2]                                                 # 프레임의 높이/너비를 얻습니다.\n",
    "Path(Path(SAVE_MP4).parent).mkdir(parents=True, exist_ok=True)             # 출력 비디오 폴더를 생성(이미 있으면 무시)합니다.\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")                                   # 호환성 좋은 mp4v 코덱을 사용합니다.\n",
    "writer = cv2.VideoWriter(SAVE_MP4, fourcc, FPS_OUT, (W, H))                # 설정한 FPS/해상도로 비디오 라이터를 생성합니다.\n",
    "assert writer.isOpened(), f\"비디오 라이터 초기화 실패: {SAVE_MP4}\"          # 라이터가 정상 오픈되었는지 확인합니다.\n",
    "\n",
    "# ===== 선택적 오버레이 이미지 저장 준비 =====\n",
    "if SAVE_OVERLAY_IMAGES:                                                    # 오버레이 이미지 파일 저장 옵션이 True인 경우\n",
    "    Path(VIS_DIR).mkdir(parents=True, exist_ok=True)                       # 저장 디렉터리를 생성합니다.\n",
    "\n",
    "# ===== 메인 루프: tqdm 진행률과 함께 오버레이 → 즉시 인코딩 =====\n",
    "last_json_data = None                                                      # 직전(JSON 홀드)을 저장할 변수를 초기화합니다.\n",
    "written_frames, skipped = 0, 0                                             # 기록된 프레임 수와 스킵 수를 초기화합니다.\n",
    "t_start = time.time()                                                      # 처리 시작 시각을 기록합니다.\n",
    "\n",
    "with tqdm(total=len(all_frames),                                           # 전체 프레임 수를 지정해 진행률 바를 생성합니다.\n",
    "          desc=\"Overlay→Encode 30fps MP4\",                                 # 진행률 바 설명을 설정합니다.\n",
    "          unit=\"frame\",                                                    # 단위를 frame으로 표시합니다.\n",
    "          dynamic_ncols=True,                                              # 터미널 너비에 따라 칼럼을 동적으로 조정합니다.\n",
    "          leave=True) as pbar:                                             # 완료 후에도 진행률 바를 남겨둡니다.\n",
    "    for fpath in all_frames:                                               # 모든 프레임을 순회하며 처리합니다.\n",
    "        base = os.path.splitext(os.path.basename(fpath))[0]                # 현재 프레임의 파일명(확장자 제외)을 구합니다.\n",
    "        src_json_path = src_json_map.get(base, None)                       # 같은 이름의 15fps JSON이 있는지 조회합니다.\n",
    "\n",
    "        if src_json_path is not None:                                      # 일치하는 JSON이 있는 경우\n",
    "            with open(src_json_path, \"r\", encoding=\"utf-8\") as f:          # JSON 파일을 텍스트 모드로 엽니다.\n",
    "                json_data = json.load(f)                                   # JSON을 파싱하여 딕셔너리로 읽습니다.\n",
    "            last_json_data = json_data                                     # 최근 JSON 캐시를 갱신합니다.\n",
    "        else:                                                              # 일치 JSON이 없는 경우\n",
    "            if last_json_data is None:                                     # 아직 어떤 JSON도 캐시되지 않은 초기 상태라면\n",
    "                json_data = None                                           # 임시 변수 초기값을 None으로 둡니다.\n",
    "                for k in sorted_json_keys:                                 # 자연 정렬된 키들을 순회합니다.\n",
    "                    if natural_key(k) > natural_key(base):                 # 현재 프레임명보다 \"이후\"에 오는 JSON을 찾습니다.\n",
    "                        with open(src_json_map[k], \"r\", encoding=\"utf-8\") as f:  # 해당 JSON 파일을 엽니다.\n",
    "                            json_data = json.load(f)                        # JSON을 파싱합니다.\n",
    "                        last_json_data = json_data                          # 캐시를 업데이트합니다.\n",
    "                        break                                              # 첫 후보를 찾았으므로 탐색을 중지합니다.\n",
    "                if json_data is None:                                      # 끝까지 못 찾은 경우(비정상 시나리오)\n",
    "                    skipped += 1                                           # 스킵 카운터를 증가시킵니다.\n",
    "                    pbar.update(1)                                         # 진행률 바를 한 칸 진행합니다.\n",
    "                    pbar.set_postfix(written=written_frames, skipped=skipped)  # 현재 누적 통계를 표기합니다.\n",
    "                    continue                                               # 다음 프레임으로 넘어갑니다.\n",
    "            else:                                                          # 직전 JSON이 있는 경우\n",
    "                json_data = last_json_data                                 # Zero-Order Hold 방식으로 재사용합니다.\n",
    "\n",
    "        img_bgr = cv2.imread(fpath)                                        # 현재 프레임 이미지를 로드합니다.\n",
    "        if img_bgr is None:                                                # 로드 실패 시\n",
    "            skipped += 1                                                   # 스킵 카운터를 증가시킵니다.\n",
    "            pbar.update(1)                                                 # 진행률 바를 한 칸 진행합니다.\n",
    "            pbar.set_postfix(written=written_frames, skipped=skipped)      # 현재 누적 통계를 표기합니다.\n",
    "            continue                                                       # 다음 프레임으로 넘어갑니다.\n",
    "\n",
    "        if img_bgr.shape[:2] != (H, W):                                    # 프레임 해상도가 기준과 다르면\n",
    "            img_bgr = cv2.resize(img_bgr, (W, H), interpolation=cv2.INTER_AREA)  # 비디오 해상도에 맞게 리사이즈합니다.\n",
    "\n",
    "        meta      = json_data.get(\"meta_info\", {})                         # 메타 정보를 안전하게 가져옵니다.\n",
    "        instances = json_data.get(\"instance_info\", [])                     # 인스턴스 리스트를 안전하게 가져옵니다.\n",
    "        links     = get_links(meta)                                        # 스켈레톤 링크 정보를 얻습니다.\n",
    "        for inst in instances:                                             # 각 인스턴스를 순회하며\n",
    "            draw_instance_uniform(img_bgr, inst, links, kpt_thr=KPT_THR)   # 키포인트와 스켈레톤을 단일 색으로 그립니다.\n",
    "\n",
    "        writer.write(img_bgr)                                              # 그려진 프레임을 즉시 비디오에 기록합니다.\n",
    "        written_frames += 1                                                # 기록된 프레임 수를 갱신합니다.\n",
    "\n",
    "        if SAVE_OVERLAY_IMAGES:                                            # 이미지 저장 옵션이 활성화되었다면\n",
    "            out_img_path = os.path.join(VIS_DIR, base + \".jpg\")            # 저장 경로를 구성합니다.\n",
    "            _ = cv2.imwrite(out_img_path, img_bgr)                         # 오버레이 이미지를 파일로 저장합니다.\n",
    "\n",
    "        elapsed = time.time() - t_start                                    # 경과 시간을 계산합니다.\n",
    "        fps_now = written_frames / max(elapsed, 1e-6)                      # 평균 처리 FPS를 계산합니다.\n",
    "        pbar.update(1)                                                     # 진행률 바를 한 칸 진행합니다.\n",
    "        pbar.set_postfix(written=written_frames, skipped=skipped, fps=f\"{fps_now:0.2f}\")  # 통계(FPS 포함)를 표시합니다.\n",
    "\n",
    "writer.release()                                                            # 비디오 라이터를 닫아 파일을 완료합니다.\n",
    "total_sec = max(time.time() - t_start, 1e-6)                                # 총 경과 시간을 계산합니다.\n",
    "avg_fps = written_frames / total_sec                                        # 전체 평균 FPS를 계산합니다.\n",
    "print(f\"완료: 비디오 저장 {SAVE_MP4}, 프레임 {written_frames}개 기록, 스킵 {skipped}개, 평균 FPS {avg_fps:0.2f}\")  # 처리 요약을 출력합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sapiens)",
   "language": "python",
   "name": "sapiens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
