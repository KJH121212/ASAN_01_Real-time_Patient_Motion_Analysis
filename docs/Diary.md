# 파일 설명
초기에는 환경, 데이터, 코드 등으로 설명하려고 했으나, 그 파일은 나중에 따로 만들기로 하고 일기처럼 작성하기로 생각함.  
해당 파일을 읽음으로서 내가 실험한 이유 및 내용을 쉽게 파악 할 수 있을거라 생각했음.  
아님 말구.
# 2025.09 첫째주.
## 이전까지 했던 일들
시작에 앞서, 오늘 이전에 있었던 일들에 대해 설명하기로 함.
### 환경
환경의 경우, 호환성 문제가 크게 있었음. 그거 때문에 거의 2주일을 날린 것 같음.  
sapiens 모델을 clone한 이후에 _install 폴더의 conda.sh를 시행했음에도, 호환성 및 보안 문제 때문에 mmpose 쪽이 잘 설치가 안되는 것을 확인할 수 있음.  
특히 opencv와 mmpose간에 사용하는 numpy 버전 호환성 문제 때문에 버전을 정확하게 명시하고 라이브러리를 설치해야했음.  
최종적으로는 **batch 폴더 내부에 batch.sh만 실행**하면 hccmove_copy.Dockerfile을 통해 가상환경을 생성하고, sapiens 모델을 실행하는데 성공함.  
이름은 바꾸기 귀찮...
나중에 바꾸었는데도 안되는 경우, sapiens 모델 clone후, conda.sh 실행. 이후 setting.ipynb내부 라이브러리 실행을 순차적으로 하면 됨.  

### data
data 폴더 내부 파일들의 읽기는 권한이 있지만, 쓰기 권한은 받지 못했음.  
따라서 전처리 후의 파일 저장은 IDs 쪽에 해야함.  
추후 Public data는 모두 삭제해야할 수도 있다는 소리를 듣고, Patient data와 Public data의 위치를 구분해 놓았음.  
또한 resize 이후 Frame 별 저장을 따로 하려고 했지만, 시간이 너무 오래걸리는 관계로 기존의 것을 사용하기로 결정함.
(대신 일부 파일들은 resize 및 frame별 분할이 안되어 있었기 때문에 이것들만 따로 해줘야 할 것 같음. 0_EDA.ipynb 파일 참고)

### Project 위치 변경
나는 Nas100에 권한을  받았길래 거기에 프로젝트 폴더를 만들고 시행하는 줄 알았는데, **데이터가 있는 폴더 내부 IDs 에 자신의 이름을 만들고 그 안에 프로젝트를 진행하는 것이었음**.
이후 해당 프로젝트 폴더 이동 후, 경로 설정에 시간 투자함.

## 이번주에 한 실험
sapiens 모델을 실험해 보니 시간이 너무 오래 걸리는 것을 확인함.  
1frame 당 1초가 걸리는데, 큰 파일의 경우 45,000 넘어가는 경우도 발견함.  
속도를 어떻게든 끌어 올려야 한다는 판단 하에 2가지 실험 진행

### 1. 해상도 변화 실험 (04_resolution_test)
해상도를 720, 360, 180, 3가지로 나누어서 속도 측정해 봄.
측정 결과, 180p는 잘못된 landmark를 찍는 것도 모자라 속도도 그렇게 크게 차이가 나지 않음을 확인.  
**해상도는 720p 그대로 사용하는 것으로 확정함**

### 2. FPS 변화 실험 (05_FPS_test)
Frame 수가 줄어드는 것에 비례해서 속도도 줄어들었음. (사실 당연한 결과)  
문제는 지워진 frame사이사이의 landmark의 보간이었음.  
가장 쉬운 방법으로 이전 landmark 그대로 복사 하는 경우, 시간은 거의 안걸렸지만, 10fps부터는 버벅거리는 느낌이 심하게 났음.  
15fps는 나름 괜찮았지만, 빠른 움직임이 있을 경우는 사용하지 못할 거라는 생각이 들었음.  
그래도 환자니까 빠른 움직임은 없지 않을까? 라는 판단을 했지만 다음 미팅때 물어봐야할 듯.  

### 3. BBox 재사용
5 프레임 단위로 BBox를 detect 하도록 변경하였음.  
덕분에 속도가 어느정도 감소한 것을 확인할 수 있었음.  
정확한 수치는 또 코드를 만들어서 비교해 봐야 할 것 같음

#### 수정 방향
1. YOLO 만 사용 (규칙기반)
가운데 있는 BBox의 가중치를 크게 늘리고, Bbox 크기도 가장 큰거  
가중치 적용이 잘 안됐음 (정규화 하면 다를 듯.)

2. YOLO + DeepSORT (추적기)
너무 오래걸림 YOLO 만 쓰면 2분 걸리는거 한 10분으로 늘어남.  
정확도도 많이 떨어짐.  

3. BBOX 고정
애초에 환자의 이동이 크지 않음. 무조건 누워 있기 때문에 굳이 Detector를 통해 Bbox를 찾는게 아니라 첫 frame에서 Bbox를 넉넉하게 주고 고정 시키는 것이 더 좋을 수도 있음.  

## 최종 코드
아래와 같은 순서로 Workflow 진행됨.
1. 원본 Frame 모두 저장 (30fps)
2. Frame중 절반(15fps)을 sapiens 모델을 통해 keypoints json 생성.(720p로 다운 샘플링은 아직 못 적용함)  
(이때, BBox detector은 5Frame 마다 다시 진행하는 형식으로 변경)
3. 생성된 Json을 이용하여 **원본화질의 30fps의 mp4 파일** 생성

여기서 더 수정할만한 포인트
1. 원본 Frame 저장도 시간을 아끼기 위해 선택적으로 할 수 있도록 변경
2. 720p downscale 이후 Model 실행
3. BBox Detector 성능 향상 (아예 외부에서 json 제공하는 형태는 어떨지 궁금함: SAM2가 BBox 치기엔 더 좋을 것 같음)



